{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35f1ea4",
   "metadata": {},
   "source": [
    "1. importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aedf3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string # For punctuation, data preprocessing\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Data encoding\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer # data preprocessing root word\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a94ce9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pstug\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e2f6a",
   "metadata": {},
   "source": [
    "Function for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6394c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize():\n",
    "    #file = open('corpus.txt','r',errors='ignore')\n",
    "    corpus = wikipedia.summary(user_query)\n",
    "    #print(corpus)\n",
    "    #corpus = file.read()\n",
    "    sentence_tokens = nltk.sent_tokenize(corpus)\n",
    "    word_tokens = nltk.word_tokenize(corpus)\n",
    "    return sentence_tokens, word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abf448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e57db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df42de14",
   "metadata": {},
   "source": [
    "Function for Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d511945",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e81044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemtokens(tokens):\n",
    "    lst = []\n",
    "    for i in tokens: # Every individual token to be lemmatized\n",
    "        lst.append(lemmatizer.lemmatize(i))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4018a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the punctuation\n",
    "\n",
    "punct = dict((ord(i),None) for i in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e81fb071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmer(text):\n",
    "    tokenized_text = nltk.word_tokenize(text.lower().translate(punct))\n",
    "    lemmatized_values = lemtokens(tokenized_text)\n",
    "    return lemmatized_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4b6c7",
   "metadata": {},
   "source": [
    "Function for greetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81fcc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_inputs = ['hello','hi','hey','greetings']\n",
    "greeting_responses = ['I am your chatbot', 'hello', 'hi','whats up', 'hi there']\n",
    "\n",
    "def greeting(text):\n",
    "    for token in text.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e7c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31549948",
   "metadata": {},
   "source": [
    "Function for generate response for the queries from the corpus!\n",
    "\n",
    "Data Encoding - TFIDF\n",
    "Similarity Metric - cosine similarity choosing the vector with maximum similarity in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50849b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(user_query):\n",
    "    bot_response=''\n",
    "    \n",
    "    #Tokenize\n",
    "    sent_tokens,word_tokens = tokenize()\n",
    "    sent_tokens.append(user_query)\n",
    "    \n",
    "    #Vectorizing\n",
    "    tfidf_obj = TfidfVectorizer(tokenizer=lemmer,stop_words=\"english\")\n",
    "    tfidf = tfidf_obj.fit_transform(sent_tokens)\n",
    "    \n",
    "    #cosine similarity\n",
    "    sim_values = cosine_similarity(tfidf[-1],tfidf) #cosine similarity of the last element with the entire list\n",
    "    \n",
    "    #selecting the response or token with max similarity\n",
    "    index = sim_values.argsort()[0][-2]\n",
    "    \n",
    "    flattened_sim = sim_values.flatten()\n",
    "    flattened_sim.sort()\n",
    "    \n",
    "    required_tfidf = flattened_sim[-2]\n",
    "    \n",
    "    if(required_tfidf==0):\n",
    "        bot_response += 'I cannot understand'\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response += bot_response + sent_tokens[index]\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c9a1d",
   "metadata": {},
   "source": [
    "main()function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62a2f69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot\n",
      "hi\n",
      "Bot: hello\n",
      "hyderabad, telangana\n",
      "Bot:  Hyderabad (  HY-dər-ə-bad; Telugu: [ˈɦaɪ̯daraːbaːd], Urdu: [ˈɦɛːdəɾaːbaːd]) is the capital and largest city of the Indian state of Telangana.\n",
      "Bye\n",
      "Bot:  I cannot understand\n",
      "exit\n",
      "Bye See You!\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot\")\n",
    "\n",
    "flag = 1\n",
    "while (flag == 1):\n",
    "    user_query = input()\n",
    "    user_query = user_query.lower()\n",
    "    \n",
    "    if (user_query == 'exit'):\n",
    "        flag = 0\n",
    "        print('Bye See You!')    \n",
    "    else:\n",
    "        if(greeting(user_query) != None):\n",
    "            print(\"Bot: \"+greeting(user_query))\n",
    "            \n",
    "        else:\n",
    "            res = respond(user_query)\n",
    "            print(\"Bot: \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2abef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f9ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab90357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dbd6b9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abb48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763a1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
